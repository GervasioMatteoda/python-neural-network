# -*- coding: utf-8 -*-
"""
python-dense-neural-network.ipynb
Automatically generated by Colaboratory.
"""

# Importar las Librerías
import math
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

# Definición de Variables
TAM_IMAGEN = 28
NRO_EPOCAS = 30
TAM_LOTE = 32
CANT_NEURONAS = 75
TAM_SALIDA = 10
clases = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

# Descargar el Set de Datos
datos, metadatos = tfds.load('mnist', as_supervised=True, with_info=True)

# Obtener las Variables
datos_entrenamiento = datos['train'] # 60k de Datos
datos_pruebas = datos['test']        # 10k de Datos

# Modificación de Datos ~ Normalizar & Almacenar en Cache
def modificar_datos (imagen, etiqueta):
  imagen = tf.cast(imagen, tf.float32) / 255
  return imagen, etiqueta

datos_entrenamiento = datos_entrenamiento.map(modificar_datos)

datos_entrenamiento = datos_entrenamiento.cache()

# Crear el Modelo
modelo = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape = (TAM_IMAGEN, TAM_IMAGEN, 1)),
    tf.keras.layers.Dense(units = CANT_NEURONAS, activation='relu'),
    tf.keras.layers.Dense(units = CANT_NEURONAS, activation='relu'),
    tf.keras.layers.Dense(TAM_SALIDA, activation='softmax')
])

# Compilar el Modelo
modelo.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

# Preparar Entrenamiento
datos_entrenamiento = datos_entrenamiento.repeat().shuffle(metadatos.splits["train"].num_examples).batch(TAM_LOTE)
datos_pruebas = datos_pruebas.batch(TAM_LOTE)

# Realizar el Entrenamiento
historial = modelo.fit(
    datos_entrenamiento,
    epochs = NRO_EPOCAS,
    steps_per_epoch = math.ceil(metadatos.splits["train"].num_examples / TAM_LOTE)
)